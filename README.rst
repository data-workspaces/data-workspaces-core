===============
Data Workspaces
===============
Data Workspaces provides a thin layer of the Git version control
system for easy management of source data, intermediate data, and results for
data science projects. A *workspace* is a Git repository with some added
metadata to track external resources and experiment history. You can create
and manipulate workspaces via ``dws``, a command line tool. There is
also a programmatic API for integrating more tightly with your data
pipeline.

A workspace contains one or more *resources*. Each resource represents
a collection of data that has a particular *role* in the project -- source
data, intermediate data (generated by processinng the original source data),
code, and results. Resources can be subdirectories in the workspace's
Git repository, separate git repositories, local directories, or remote
systems (e.g. an S3 bucket or a remote server's files accessed via ssh).

Once the assets of a data science project have been organized into
resources, one can do the work of developing the associated software and
running experiments. At any point in time, you can take a *snapshot*, which
captures the current state of all the resources referenced by the workspace.
If you want to go back to a prior state of the workspace or even an individual
resource, you can *restore* back to any prior snapshot.

*Results resources* are handled a little differently than other types: they
are always additive. Each snapshot of a results resource takes the current files
in the resource and moves it to a snapshot-specific subdirectory. This lets you
view and compare the results of all your prior experiements.

Building on Git, a data workspace can be synced with a remote copy, called the *origin*.
The data workspace command line tool, ``dws``, provides ``push`` and ``pull`` commands,
similar to their Git analogs. In addition to the workspace itself, these commands can sync
all the resources referenced by the workspace. Finally, there is a ``clone`` command which can initialize
your environment on a new machine.

Taken together, these features let you:

1. Track and version all the different resources for your data science project
   from one place.
2. Automatically track the full history of your experimental results. Scripts can easily be
   developed to build reports on these results.
3. Reproduce any prior experiment, including the source data, code, and configuration parameters used.
4. Go back to a prior experiment as a "branching-off" point to explore additional permuations.
5. Collaborate with others on the same project, sharing data, code, and results.
6. Easily reproduce your environment on a new machine to parallelize work.
7. Publish your environment on a site like GitHub or GitLab for others to download and explore.



To install for development
==========================
First, create a virtual environment. If you are using Anaconda3,
here are the steps::

    conda create --name dws

To activate the environment::

    source activate dws

Now, install the data workspaces library via pip::

    pip install --editable `pwd`

You can edit the files directly in your git repo -- the changes will
be reflected when you run the commands.

Command Line Interface
======================
To run the command line interface, you use the ``dws`` command,
which should have been installed into your environment by ``pip install``.
``dws`` operations have the form::

    dws [GLOBAL_OPTIONS] SUBCOMMAND [SUBCOMMAND_OPTIONS] [SUBCOMMAND_ARGS]

Just run ``dws --help`` for a list of global options and subcommands.

Here is a summary of the key subcommands:

* ``init`` - initialize a new workspace in the current directory
* ``add`` - add a *resource* (a git repo, a directory, an s3 bucket, etc.)
  to the current workspace
* ``snapshot`` - take a snapshot of the current state of the workspace
* ``restore`` - restore the state to a prior snapshot
* ``push`` - push a workspace and all resources to their (remote) origins
* ``pull`` - pull the workspace and all resources from their (remote) origins
* ``clone`` - clone a workspace and all the associated resources to the local machine
* ``run`` - run a command and capture the lineage. This information is saved in a file for
  future calls to the same command. *(not yet fully implemented)*

Design
======
A *workspace* contains references to *resources*, where a resource is a versioned
collection of files that may be stored as a local directory, a git repository,
an S3 bucket, etc. A *snapshot* is a particular state of all the resources
that can be restored upon request. The metadata about resources and snapshots
is stored in JSON files, in the subdirectory ``.dataworkspace`` under the root
directory of a given workspace.

Commands
--------
Each command has a *validation* phase and an *execution* phase. The goal is to
do all the checks up front before making any changes to the state of the
resources or the workspace. This is supported by the ``Action`` class
and associated infrastructure (`see below <actions>`__).

Snapshot
~~~~~~~~
Taking a snapshot involves instantiating resource objects for each resource
in resources.json and calling ``snapshot_prechecks()`` and ``snapshot()``.

Restore
~~~~~~~
Restore has some options to let you specify which resources to restore
and which to leave in their current state (``--only`` and ``--leave``). Restore may
create a new snapshot if the state of the resources does not exactly match
the original snapshot's state. If ``--no-new-snapshot`` is
specified, we adjust the individual resource
states without taking a new snapshot.

To implement restore for a new resource type, you just need to implement the
``restore_prechecks()`` and ``restore()`` methods. Both take a hashval parameter. In the
``restore_prechecks()`` call, you should validate that there is a state corresponding
to that hash.

There are a few edge cases that may need further thought:

* It is possible for the restore command to create a snapshot matching a previous one. We detect this situation, but don't do anything about it. It should be fine - there will just be an extra snapshot_history entry, but only one snapshot file.
* The restore for the git resource does a hard reset, which resets both the current workspace of the repo and the HEAD. I'm not sure whether we want that behavior or just to reset the workspace.

Resource Roles
--------------
A resource (collection of files) may have one of four roles:

1. **Source Data Set** - this should be treated read-only by the ML
   pipeline. Source data sets can be versioned.
2. **Intermediate Data** - derived data created from the source data set(s)
   via one or more data pipeline stages.
3. **Results** - the outputs of the machine learning / data science process.
4. **Code** - code used to create the intermediate data and results, typically
   in a git repository or Docker container.

The treatment of resources may vary based on the role. We now look at
resource functionality per role.

Source Data Sets
~~~~~~~~~~~~~~~~
We want the ability to name source data sets and swap them in and out without
changing other parts of the workspace. This still needs to be implemented.

Intrermediate Data
~~~~~~~~~~~~~~~~~~
For intermediate data, we may want to delete it from the current state of
the workspace if it becomes out of date (e.g. a data source version is changed
or swapped out). This still needs to be implemented.

Results
~~~~~~~
In general, results should be additive.

For the ``snapshot`` command, we move the results to a specific subdirectory per
snapshot. The name of this subdirectory is determined by a template that can
be changed by setting the parameter ``results.subdir``. By default, the template
is: ``{DAY}/{DATE_TIME}-{USER}-{TAG}``. The moving of files is accomplished via the
method ``results_move_current_files(rel_path, exclude)`` on the `Resource <resources>`
class. The ``snapshot()`` method of the resource is still called as usual, after
the result files have been moved.

Individual files may be excluded from being moved to a subdirectory. This is done
through a configuration command. Need to think about where this would be stored --
in the resources.json file? The files would be passed in the exclude set to
``results_move_current_files``.

If we run ``restore`` to revert the workspace to an
older state, we should not revert the results database. It should always
be kept at the latest version. This is done by always putting results
resources into the leave set, as if specified in the ``--leave`` option.
If the user puts a results resource in the ``--only`` set, we will error
out for now.


Code Organization
-----------------
We use the Python library ``click`` (http://click.pocoo.org/6/) to implement
the command argument parsing. The implementations of individual commands
may be found in the ``commands/`` subdirectory.

.. _actions:
Actions
~~~~~~~
We wish to perform all the
checks of a command up front and then only run the steps when we know they
will succeed. This is done through *actions*, as defined in ``commands/actions.py``.
Each ``Action`` subclass performs any necesary checks in its ``__init__()`` method.
The actual execution of the action is in the ``run()`` method. Commands instantiate
the actions they need, add them to a list (called the *plan*), and when all
checks have been performed, execute the actions via the function
``actions.run_plan()``. When running in verbose mode, we also print the
list of actions to perform and ask the user for confirmation.

.. _resources:
Resources
~~~~~~~~~
Resources are orthoginal to actions and represent the collections of
files to be versioned.

Example Workflows
=================
Here are a few example workflows using the command line interface.
Lines with user input start with the shell prompt ``$``.

First, we create our workspace and define our resources
(a remote s3 bucket, a local git repo and two subdirectories):

.. code:: bash

   $ cd /home/joe/example-workspace
   $ dws init
   Created workspace 'example-workspace'.
   $ dws add source-data s3://data-bucket
   Added s3 resource 'data-bucket' as source data.
   $ dws add code ./myrepo
   Added git resource './myrepo' as code.
   $ dws add intermediate-data ./intermediate
   Added local resource './intermediate' as intermediate data.
   $ dws add results ./results
   Added local resource './results' as result data.
   $ dws set-hook merge merge-json ./results/results.csv

The last line indicates that, when we take a snapshot, we merge ``results.csv`` with
the previous version, creating a combined csv file that includes all the results.
By default, overwriting a results file will cause the previous version to be renamed
upon taking the snapshot (e.g. the previous version becomes results.csv.v1 if the previous
snapshot was tagged with "v1").

Now, we can run our scripts and then take a snapshot:

.. code:: bash

   $ python ./myrepo/extract_features.py -o ./intermediate/features.csv s3://data-bucket
   $ python ./myrepo/train.py --solver=SVC ./intermediate/features.csv ./results/results.csv
   $ dws snapshot v1
   Created snapshot with hash '34A440983F' and tag 'v1'.

If we list the local files in our workspace at this point, we see:

.. code:: bash

   $ ls -R
   ./intermediate:
   features.csv

   ./myrepo:
   extract_featues.py            train.py

   ./results:
   results.csv

We make some changes to the code, do another run, and take a second snapshot:

.. code:: bash

   $ cd myrepo; vi extract_features.py
   $ git add extract_features.py; git commit -m "some changes to feature extraction"
   $ cd ..
   $ python ./myrepo/extract_features.py -o ./intermediate/features.csv s3://data-bucket
   $ python ./myrepo/train.py --solver=SVC ./intermediate/features.csv ./results/results.csv
   $ dws snapshot v2
   Created snapshot with hash 'FF83830484' and tag 'v2'.

Let's say we wanted to go back to the previous version, but run with a different solver.
We do not need to rerun the first step, as the intermediate data has been restored
as well.

.. code:: bash

   $ dws revert v1
   Reverted to snapshot with hash '34A440983F' and tag 'v1'.
   $ python ./myrepo/train.py --solver=SVC ./intermediate/features.csv ./results/results.csv
   $ dws snapshot v3
   Created snapshot with hash 'A3838492B3' and tag 'v3'.

License
=======
This code is copyright 2018 by the Max Planck Institute for Software Systems and Data-ken
Research. It is licensed under the Apache 2.0 license. See the file LICENSE.txt for details.
